{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "The goal of this homework is to familiarize users with workflow orchestration. We start from the solution of homework 1. The notebook can be found below:\n",
    "\n",
    "https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/01-intro/homework.ipynb\n",
    "\n",
    "This has already been converted to a script called `homework.py` in the `03-orchestration` folder of this repo. \n",
    "\n",
    "You will use the FHV dataset like in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefect                       2.0b5\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Converting the script to a Prefect flow\n",
    "\n",
    "We want to bring this to workflow orchestration to add observability around it. The `main` function will be converted to a `flow` and the other functions will be `tasks`. After adding all of the decorators, there is actually one task that you will need to call `.result()` for inside the `flow` to get it to work. Which task is this?\n",
    "\n",
    "* `read_data`\n",
    "* `prepare_features`\n",
    "* `train_model`\n",
    "* `run_model`\n",
    "\n",
    "Important: change all `print` statements to use the Prefect logger. Using the `print` statement will not appear in the Prefect UI. You have to call `get_run_logger` at the start of the task to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    df_train_processed = prepare_features(df_train, categorical)\n",
      "\n",
      "    df_val = read_data(val_path)\n",
      "    df_val_processed = prepare_features(df_val, categorical, False)\n",
      "\n",
      "    # train the model\n",
      "    lr, dv = train_model(df_train_processed, categorical).result()\n",
      "    run_model(df_val_processed, categorical, dv, lr)\n",
      "\n",
      "main()\n"
     ]
    }
   ],
   "source": [
    "!tail homework_olegtaratuhin.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Parameterizing the flow\n",
    "\n",
    "Right now there are two parameters for `main()` called `train_path` and `val_path`. We want to change the flow function to accept `date` instead. `date` should then be passed to a task that gives both the `train_path` and `val_path` to use.\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "```python\n",
    "@flow\n",
    "def main(date=None):\n",
    "    train_path, val_path = get_paths(date).result()\n",
    "    # rest of flow below\n",
    "\n",
    "main(date=\"2021-03-15\")\n",
    "```\n",
    "\n",
    "The flow will take in a parameter called `date` which will be a datetime.\n",
    "    a. `date` should default to None\n",
    "    b. If `date` is None, use the current day. Use the data from 2 months back as the training data and the data from the previous month as validation data.\n",
    "    c. If a `date` value is supplied, get 2 months before the `date` as the training data, and the previous month as validation data.\n",
    "    d. As a concrete example, if the date passed is \"2021-03-15\", the training data should be \"fhv_tripdata_2021-01.parquet\" and the validation file will be \"fhv_trip_data_2021-02.parquet\"\n",
    "\n",
    "What is the validation MSE when running the flow with this date?\n",
    "\n",
    "Note you need to download the relevant files to run. Part of this question is understanding which files the flow should be looking for.\n",
    "\n",
    "The valition MSE is:\n",
    "\n",
    "* 11.637\n",
    "* 11.837\n",
    "* 12.037\n",
    "* 12.237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:14.622 | INFO    | prefect - The MSE of training is: 11.789353642873099\n",
      "17:47:17.081 | INFO    | prefect - The MSE of validation is: 11.637028658288816\n"
     ]
    }
   ],
   "source": [
    "!cd .. && python 03-orchestration/homework_olegtaratuhin.py 2>&1 | grep MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Saving the model and artifacts\n",
    "\n",
    "* Save the model as \"model-{date}.pkl\" where date is in `YYYY-MM-DD`. Note that `date` here is the value of the flow `parameter`. In practice, this setup makes it very easy to get the latest model to run predictions because you just need to get the most recent one.\n",
    "* In this example we use a DictVectorizer. That is needed to run future data through our model. Save that as \"dv-{date}.pkl\". Similar to above, if the date is `2021-03-15`, the files output should be `model-2021-03-15.bin` and `dv-2021-03-15.b`.\n",
    "\n",
    "By using this file name, during inference, we can just pull the latest model from our model directory and apply it. Assuming we already had a list of filenames:\n",
    "\n",
    "```python\n",
    "['model-2021-03-15.bin', 'model-2021-04-15.bin', 'model-2021-05-15.bin']\n",
    "```\n",
    "\n",
    "We could do something like `sorted(model_list, reverse=False)[0]` to get the filename of the latest file. This is the simplest way to consistently use the latest trained model for inference. Tools like MLFlow give us more control logic to use flows.\n",
    "\n",
    "What is the file size of the `DictVectorizer` that we trained when the `date` is 2021-08-15?\n",
    "\n",
    "* 13,000 bytes \n",
    "* 23,000 bytes \n",
    "* 33,000 bytes \n",
    "* 43,000 bytes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13191"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.getsize(\"../models/dv-2021-08-15.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Creating a deployment with a CronSchedule\n",
    "\n",
    "For this exercise, use a `CronSchedule` when creating a Prefect deployment.\n",
    "\n",
    "What is the Cron expression to run a flow at 9 AM every 15th of the month?\n",
    "\n",
    "* `* * 15 9 0`\n",
    "* `9 15 * * *`\n",
    "* `0 9 15 * *`\n",
    "* `0 15 9 1 *`\n",
    "\n",
    "Hint: there are many Cron to English tools. Try looking for one to help you.\n",
    "\n",
    "Create a deployment with `prefect deployment create` after you write your `DeploymentSpec`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeploymentSpec(name='hw-03', flow=<prefect.flows.Flow object at 0x105159cf0>, flow_name=None, flow_location=None, flow_storage=None, parameters=None, schedule=CronSchedule(cron='0 9 15 * *', timezone=None, day_or=True), tags=['ml'], flow_runner=SubprocessFlowRunner(typename='subprocess', env={}, stream_output=True, condaenv=None, virtualenv=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    pass\n",
    "\n",
    "\n",
    "import prefect\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "from prefect.orion.schemas.schedules import CronSchedule\n",
    "\n",
    "DeploymentSpec(\n",
    "    flow=prefect.flow(main),\n",
    "    name=\"hw-03\",\n",
    "    flow_runner=SubprocessFlowRunner(),\n",
    "    schedule=CronSchedule(cron=\"0 9 15 * *\"),\n",
    "    tags=[\"ml\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Viewing the Deployment \n",
    "\n",
    "View the deployment in the UI. When first loading, we may not see that many flows because the default filter is 1 day back and 1 day forward. Remove the filter for 1 day forward to see the scheduled runs. \n",
    "\n",
    "How many flow runs are scheduled by Prefect in advanced? You should not be counting manually. There is a number of upcoming runs on the top right of the dashboard.\n",
    "\n",
    "* 0\n",
    "* 3\n",
    "* 10\n",
    "* 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "!echo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Creating a work-queue\n",
    "\n",
    "In order to run this flow, you will need an agent and a work queue. Because we scheduled our flow or every month, it won't really get picked up by an agent. For this exercise, create a work-queue from the UI and view it using the CLI. \n",
    "\n",
    "For all CLI commands with Prefect, you can use `--help` to get more information. \n",
    "\n",
    "For example,\n",
    "* `prefect --help`\n",
    "* `prefect work-queue --help`\n",
    "\n",
    "What is the command to view the available work-queues?\n",
    "\n",
    "* `prefect work-queue inspect`\n",
    "* `prefect work-queue ls`\n",
    "* `prefect work-queue preview`\n",
    "* `prefect work-queue list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                            Work Queues                             \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                                  ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcurrency Limit\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m522f6e1c-e0b8-4d2f-bc1b-6cd0ff3cc07d\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlocal\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34mNone\u001b[0m\u001b[34m             \u001b[0m\u001b[34m \u001b[0m│\n",
      "└──────────────────────────────────────┴───────┴───────────────────┘\n",
      "\u001b[31m                    (**) denotes a paused queue                     \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect work-queue ls"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec9a6f6dd64124b630e7404b24e13e455dcdcddef5b9b5adf6def4dc63e107"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
